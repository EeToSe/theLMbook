{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aburkov/theLMbook/blob/main/count_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy0zjL_2ouOU",
        "outputId": "969a3b02-4167-4fd2-89d5-61d062152be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading corpus from https://www.thelmbook.com/data/brown...\n",
            "Decompressing and reading the corpus...\n",
            "Corpus size: 6185606 characters\n",
            "Training the model...\n",
            "\n",
            "Perplexity on test corpus: 302.08\n",
            "\n",
            "Context: i will build a\n",
            "Next token: wall\n",
            "Generated text: i will build a wall to keep the people in and added so long\n",
            "\n",
            "Context: the best place to\n",
            "Next token: live\n",
            "Generated text: the best place to live in 30 per cent to get happiness for yourself\n",
            "\n",
            "Context: she was riding a\n",
            "Next token: horse\n",
            "Generated text: she was riding a horse and showing a dog are very similar your aids\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import re  # For regular expressions (text tokenization)\n",
        "import requests  # For downloading the corpus\n",
        "import gzip  # For decompressing the downloaded corpus\n",
        "import io  # For handling byte streams\n",
        "import math  # For mathematical operations (log, exp)\n",
        "import random  # For random number generation\n",
        "from collections import defaultdict  # For efficient dictionary operations\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Sets random seeds for reproducibility.\n",
        "\n",
        "    Args:\n",
        "        seed (int): Seed value for the random number generator\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "def download_corpus(url):\n",
        "    \"\"\"\n",
        "    Downloads and decompresses a gzipped corpus file from the given URL.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL of the gzipped corpus file\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded text content of the corpus\n",
        "\n",
        "    Raises:\n",
        "        HTTPError: If the download fails\n",
        "    \"\"\"\n",
        "    print(f\"Downloading corpus from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raises an exception for bad HTTP responses\n",
        "\n",
        "    print(\"Decompressing and reading the corpus...\")\n",
        "    with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as f:\n",
        "        corpus = f.read().decode('utf-8')\n",
        "\n",
        "    print(f\"Corpus size: {len(corpus)} characters\")\n",
        "    return corpus\n",
        "\n",
        "class CountLanguageModel:\n",
        "    \"\"\"\n",
        "    Implements an n-gram language model using count-based probability estimation.\n",
        "    Supports variable context lengths up to n-grams.\n",
        "    \"\"\"\n",
        "    def __init__(self, n):\n",
        "        \"\"\"\n",
        "        Initialize the model with maximum n-gram length.\n",
        "\n",
        "        Args:\n",
        "            n (int): Maximum length of n-grams to use\n",
        "        \"\"\"\n",
        "        self.n = n  # Maximum n-gram length\n",
        "        self.ngram_counts = [{} for _ in range(n)]  # List of dictionaries for each n-gram length\n",
        "        self.total_unigrams = 0  # Total number of tokens in training data\n",
        "\n",
        "    def predict_next_token(self, context):\n",
        "        \"\"\"\n",
        "        Predicts the most likely next token given a context.\n",
        "        Uses backoff strategy: tries largest n-gram first, then backs off to smaller n-grams.\n",
        "\n",
        "        Args:\n",
        "            context (list): List of tokens providing context for prediction\n",
        "\n",
        "        Returns:\n",
        "            str: Most likely next token, or None if no prediction can be made\n",
        "        \"\"\"\n",
        "        for n in range(self.n, 1, -1):  # Start with largest n-gram, back off to smaller ones\n",
        "            if len(context) >= n - 1:\n",
        "                context_n = tuple(context[-(n - 1):])  # Get the relevant context for this n-gram\n",
        "                counts = self.ngram_counts[n - 1].get(context_n)\n",
        "                if counts:\n",
        "                    return max(counts.items(), key=lambda x: x[1])[0]  # Return most frequent token\n",
        "        # Backoff to unigram if no larger context matches\n",
        "        unigram_counts = self.ngram_counts[0].get(())\n",
        "        if unigram_counts:\n",
        "            return max(unigram_counts.items(), key=lambda x: x[1])[0]\n",
        "        return None\n",
        "\n",
        "    def get_probability(self, token, context):\n",
        "        \"\"\"\n",
        "        Calculates probability of a token given a context.\n",
        "        Uses backoff strategy and returns small probability for unseen events.\n",
        "\n",
        "        Args:\n",
        "            token (str): Token to calculate probability for\n",
        "            context (tuple): Tuple of tokens providing context\n",
        "\n",
        "        Returns:\n",
        "            float: Probability of the token given the context\n",
        "        \"\"\"\n",
        "        # Try each n-gram size, starting from the largest\n",
        "        for n in range(self.n, 1, -1):\n",
        "            # Check if we have enough context for this n-gram size\n",
        "            if len(context) >= n - 1:\n",
        "                # Get the most recent n-1 tokens as context\n",
        "                context_n = tuple(context[-(n - 1):])\n",
        "                # Look up the counts for this context\n",
        "                counts = self.ngram_counts[n - 1].get(context_n)\n",
        "                if counts:\n",
        "                    # Calculate total occurrences of this context\n",
        "                    total = sum(counts.values())\n",
        "                    # Get count of the specific token following this context\n",
        "                    count = counts.get(token, 0)\n",
        "                    if count > 0:\n",
        "                        # Return maximum likelihood estimate P(token|context) = count(context,token)/count(context)\n",
        "                        return count / total\n",
        "\n",
        "        # If no larger context matches, back off to unigram probability\n",
        "        unigram_counts = self.ngram_counts[0].get(())\n",
        "        if unigram_counts:\n",
        "            count = unigram_counts.get(token, 0)\n",
        "            if count > 0:\n",
        "                # Return unigram probability P(token) = count(token)/total_tokens\n",
        "                return count / self.total_unigrams\n",
        "\n",
        "        # Return small probability for unseen events (prevents zero probabilities)\n",
        "        return 1e-6\n",
        "\n",
        "def train(model, tokens):\n",
        "    \"\"\"\n",
        "    Trains the language model by counting n-grams in the training data.\n",
        "\n",
        "    Args:\n",
        "        model (CountLanguageModel): Model to train\n",
        "        tokens (list): List of tokens from the training corpus\n",
        "    \"\"\"\n",
        "    # Train models for each n-gram size from 1 to n\n",
        "    for n in range(1, model.n + 1):\n",
        "        counts = model.ngram_counts[n - 1]\n",
        "        # Slide a window of size n over the corpus\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            # Split into context (n-1 tokens) and next token\n",
        "            context = tuple(tokens[i:i + n - 1])\n",
        "            next_token = tokens[i + n - 1]\n",
        "\n",
        "            # Initialize counts dictionary for this context if needed\n",
        "            if context not in counts:\n",
        "                counts[context] = defaultdict(int)\n",
        "\n",
        "            # Increment count for this context-token pair\n",
        "            counts[context][next_token] = counts[context][next_token] + 1\n",
        "\n",
        "    # Store total number of tokens for unigram probability calculations\n",
        "    model.total_unigrams = len(tokens)\n",
        "\n",
        "def generate_text(model, context, num_tokens):\n",
        "    \"\"\"\n",
        "    Generates text by repeatedly sampling from the model.\n",
        "\n",
        "    Args:\n",
        "        model (CountLanguageModel): Trained language model\n",
        "        context (list): Initial context tokens\n",
        "        num_tokens (int): Number of tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text including initial context\n",
        "    \"\"\"\n",
        "    # Start with the provided context\n",
        "    generated = list(context)\n",
        "\n",
        "    # Generate new tokens until we reach the desired length\n",
        "    while len(generated) - len(context) < num_tokens:\n",
        "        # Use the last n-1 tokens as context for prediction\n",
        "        next_token = model.predict_next_token(generated[-(model.n-1):])\n",
        "        generated.append(next_token)\n",
        "\n",
        "        # Stop if we've generated enough tokens AND found a period\n",
        "        # This helps ensure complete sentences\n",
        "        if len(generated) - len(context) >= num_tokens and next_token == '.':\n",
        "            break\n",
        "\n",
        "    # Join tokens with spaces to create readable text\n",
        "    return ' '.join(generated)\n",
        "\n",
        "def compute_perplexity(model, tokens, context_size):\n",
        "    \"\"\"\n",
        "    Computes perplexity of the model on given tokens.\n",
        "\n",
        "    Args:\n",
        "        model (CountLanguageModel): Trained language model\n",
        "        tokens (list): List of tokens to evaluate on\n",
        "        context_size (int): Maximum context size to consider\n",
        "\n",
        "    Returns:\n",
        "        float: Perplexity score (lower is better)\n",
        "    \"\"\"\n",
        "    # Handle empty token list\n",
        "    if not tokens:\n",
        "        return float('inf')\n",
        "\n",
        "    # Initialize log likelihood accumulator\n",
        "    total_log_likelihood = 0\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    # Calculate probability for each token given its context\n",
        "    for i in range(num_tokens):\n",
        "        # Get appropriate context window, handling start of sequence\n",
        "        context_start = max(0, i - context_size)\n",
        "        context = tuple(tokens[context_start:i])\n",
        "        token = tokens[i]\n",
        "\n",
        "        # Get probability of this token given its context\n",
        "        probability = model.get_probability(token, context)\n",
        "\n",
        "        # Add log probability to total (using log for numerical stability)\n",
        "        total_log_likelihood += math.log(probability)\n",
        "\n",
        "    # Calculate average log likelihood\n",
        "    average_log_likelihood = total_log_likelihood / num_tokens\n",
        "\n",
        "    # Convert to perplexity: exp(-average_log_likelihood)\n",
        "    # Lower perplexity indicates better model performance\n",
        "    perplexity = math.exp(-average_log_likelihood)\n",
        "    return perplexity\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenizes text into words and periods.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to tokenize\n",
        "\n",
        "    Returns:\n",
        "        list: List of lowercase tokens matching words or periods\n",
        "    \"\"\"\n",
        "    return re.findall(r\"\\b[a-zA-Z0-9]+\\b|[.]\", text.lower())\n",
        "\n",
        "def download_and_prepare_data(data_url):\n",
        "    \"\"\"\n",
        "    Downloads and prepares training and test data.\n",
        "\n",
        "    Args:\n",
        "        data_url (str): URL of the corpus to download\n",
        "\n",
        "    Returns:\n",
        "        tuple: (training_tokens, test_tokens) split 90/10\n",
        "    \"\"\"\n",
        "    # Download and extract the corpus\n",
        "    corpus = download_corpus(data_url)\n",
        "\n",
        "    # Convert text to tokens\n",
        "    tokens = tokenize(corpus)\n",
        "\n",
        "    # Split into training (90%) and test (10%) sets\n",
        "    split_index = int(len(tokens) * 0.9)\n",
        "    train_corpus = tokens[:split_index]\n",
        "    test_corpus = tokens[split_index:]\n",
        "\n",
        "    return train_corpus, test_corpus\n",
        "\n",
        "def get_hyperparameters():\n",
        "    \"\"\"\n",
        "    Returns model hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        int: Size of n-grams to use in the model\n",
        "    \"\"\"\n",
        "    n = 5\n",
        "    return n\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize random seeds for reproducibility\n",
        "    set_seed(42)\n",
        "    n = get_hyperparameters()\n",
        "\n",
        "    # Download and prepare the Brown corpus\n",
        "    data_url = \"https://www.thelmbook.com/data/brown\"\n",
        "    train_corpus, test_corpus = download_and_prepare_data(data_url)\n",
        "\n",
        "    # Train the model and evaluate its performance\n",
        "    print(\"Training the model...\")\n",
        "    model = CountLanguageModel(n)\n",
        "    train(model, train_corpus)\n",
        "\n",
        "    # Calculate and display test perplexity\n",
        "    perplexity = compute_perplexity(model, test_corpus, n)\n",
        "    print(f\"\\nPerplexity on test corpus: {perplexity:.2f}\")\n",
        "\n",
        "    # Test the model with some example contexts\n",
        "    contexts = [\n",
        "        \"i will build a\",\n",
        "        \"the best place to\",\n",
        "        \"she was riding a\"\n",
        "    ]\n",
        "\n",
        "    # Generate completions for each context\n",
        "    for context in contexts:\n",
        "        tokens = tokenize(context)\n",
        "        next_token = model.predict_next_token(tokens)\n",
        "        print(f\"\\nContext: {context}\")\n",
        "        print(f\"Next token: {next_token}\")\n",
        "        print(f\"Generated text: {generate_text(model, tokens, 10)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s61ovCwawq3f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtVopA3p4nMpqTvj9/248v",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}